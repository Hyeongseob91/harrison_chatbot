# ğŸ§  LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜

> RAG Chatbotì˜ í•µì‹¬ AI ì—”ì§„ - LangGraph ì›Œí¬í”Œë¡œìš° ë° ì„œë¹„ìŠ¤ ê³„ì¸µ

## ğŸ“‹ ê°œìš”

ì´ í´ë”ëŠ” LangGraph ê¸°ë°˜ì˜ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•œ í•µì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
Clean Architecture ì›ì¹™ì„ ë”°ë¼ **Domain - Application - Infrastructure** ê³„ì¸µìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë©°,
ëª¨ë“  ë…¸ë“œëŠ” **ë¹„ë™ê¸°(async)** ë¡œ êµ¬í˜„ë˜ì–´ ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### í•µì‹¬ ì„¤ê³„ ì›ì¹™

- **Clean Architecture**: ê³„ì¸µ ë¶„ë¦¬ë¡œ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í™•ë³´
- **Port-Adapter íŒ¨í„´**: ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„±ì„ ì¸í„°í˜ì´ìŠ¤ë¡œ ì¶”ìƒí™”
- **Dependency Injection**: Factory íŒ¨í„´ìœ¼ë¡œ ì„œë¹„ìŠ¤ ìƒì„± ë° ì£¼ì…
- **ë¹„ë™ê¸° ì²˜ë¦¬**: ëª¨ë“  ë…¸ë“œì™€ ì„œë¹„ìŠ¤ë¥¼ asyncë¡œ êµ¬í˜„

## ğŸ“ í´ë” êµ¬ì¡°

```
app/
â”œâ”€â”€ main.py                   # LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ë° ì‹¤í–‰
â”‚
â”œâ”€â”€ domain/                   # ë„ë©”ì¸ ê³„ì¸µ (ìˆœìˆ˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)
â”‚   â”œâ”€â”€ entities.py           # Document, Message, User ì—”í‹°í‹°
â”‚   â”œâ”€â”€ value_objects.py      # QueryType, RetrievalStrategy ê°’ ê°ì²´
â”‚   â””â”€â”€ exceptions.py         # ë„ë©”ì¸ ì˜ˆì™¸ ì •ì˜
â”‚
â”œâ”€â”€ nodes/                    # Application ê³„ì¸µ (LangGraph ë…¸ë“œ - Use Case êµ¬í˜„)
â”‚   â”œâ”€â”€ graph_state/
â”‚   â”‚   â”œâ”€â”€ schemas.py        # ChatState TypedDict (LangGraph ìƒíƒœ ìŠ¤í‚¤ë§ˆ)
â”‚   â”‚   â””â”€â”€ reducers.py       # State ì—…ë°ì´íŠ¸ ë¡œì§ (ìƒíƒœ ë³€í™˜ í•¨ìˆ˜)
â”‚   â”‚
â”‚   â”œâ”€â”€ chat_process/         # ì±„íŒ… ì²˜ë¦¬ ê´€ë ¨ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ user_input_node.py    # ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ response_node.py      # â­ï¸ LLM ë‹µë³€ ìƒì„± ë…¸ë“œ (GPT í˜¸ì¶œ)
â”‚   â”‚   â””â”€â”€ history_node.py       # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ë…¸ë“œ
â”‚   â”‚
â”‚   â”œâ”€â”€ source_process/       # ë¬¸ì„œ ê²€ìƒ‰ ë° ì²˜ë¦¬ ê´€ë ¨ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ retrieval_node.py     # Vector DBì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
â”‚   â”‚   â”œâ”€â”€ reranking_node.py     # ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”
â”‚   â”‚   â””â”€â”€ synthesis_node.py     # ì»¨í…ìŠ¤íŠ¸ í•©ì„± ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
â”‚   â”‚
â”‚   â””â”€â”€ supervisor/           # Supervisor Agent (í–¥í›„ í™•ì¥)
â”‚       â””â”€â”€ supervisor_node.py    # ë¼ìš°íŒ… ë° ì˜ì‚¬ê²°ì • ë…¸ë“œ
â”‚
â”œâ”€â”€ services/                 # Infrastructure ê³„ì¸µ (ì™¸ë¶€ ì„œë¹„ìŠ¤ í†µì‹ )
â”‚   â”œâ”€â”€ interfaces/           # Port (ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤)
â”‚   â”‚   â”œâ”€â”€ llm_interface.py        # ABC ê¸°ë°˜ LLM ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â”œâ”€â”€ embedding_interface.py  # ABC ê¸°ë°˜ Embedding ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â”œâ”€â”€ vector_store_interface.py # ABC ê¸°ë°˜ Vector Store ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â””â”€â”€ document_interface.py   # ABC ê¸°ë°˜ Document ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚
â”‚   â”œâ”€â”€ implementations/      # Adapter (êµ¬ì²´ êµ¬í˜„ì²´)
â”‚   â”‚   â”œâ”€â”€ openai_llm_service.py      # OpenAI GPT-OSS-20B êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ openai_embedding_service.py # OpenAI Embedding êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ qdrant_service.py          # Qdrant Vector DB CRUD êµ¬í˜„
â”‚   â”‚   â””â”€â”€ document_service.py        # ë¬¸ì„œ íŒŒì‹± ë° ì²˜ë¦¬ êµ¬í˜„
â”‚   â”‚
â”‚   â””â”€â”€ factory.py            # Factory Patternìœ¼ë¡œ ì„œë¹„ìŠ¤ ìƒì„±
â”‚
â”œâ”€â”€ utils/                    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë° ë„êµ¬
â”‚   â”œâ”€â”€ logger.py             # structlog ê¸°ë°˜ êµ¬ì¡°í™” ë¡œê¹…
â”‚   â”œâ”€â”€ parser.py             # ë¬¸ì„œ íŒŒì‹± ìœ í‹¸ (PDF, TXT, DOCX)
â”‚   â”œâ”€â”€ decorators.py         # ì¬ì‹œë„, íƒ€ì´ë°, ì—ëŸ¬ í•¸ë“¤ë§ ë°ì½”ë ˆì´í„°
â”‚   â””â”€â”€ validators.py         # ì…ë ¥ ê²€ì¦ ìœ í‹¸
â”‚
â””â”€â”€ api/                      # Presentation ê³„ì¸µ (FastAPI API ì„œë²„)
    â”œâ”€â”€ main.py               # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
    â”œâ”€â”€ routes/
    â”‚   â”œâ”€â”€ chat.py           # POST /chat/invoke, /chat/stream
    â”‚   â””â”€â”€ documents.py      # POST /documents/upload, GET /documents
    â”œâ”€â”€ schemas/
    â”‚   â”œâ”€â”€ request.py        # Pydantic Request ëª¨ë¸
    â”‚   â””â”€â”€ response.py       # Pydantic Response ëª¨ë¸
    â”œâ”€â”€ middleware.py         # CORS, ì—ëŸ¬ í•¸ë“¤ë§, ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
    â””â”€â”€ docs/
        â””â”€â”€ openapi.yaml      # OpenAPI ëª…ì„¸ì„œ
```

## ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš°

### ì „ì²´ ë°ì´í„° í”Œë¡œìš°

```
START
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ user_input_node                             â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ - ì…ë ¥ ê²€ì¦ (ë¹ˆ ë¬¸ìì—´, ê¸¸ì´ ì œí•œ)            â”‚
â”‚ - SQL Injection, XSS ê³µê²© ë°©ì§€               â”‚
â”‚ - ì¿¼ë¦¬ íƒ€ì… ë¶„ì„ (ì‚¬ì‹¤í˜•/ì˜ê²¬í˜•)              â”‚
â”‚ - state["query"] ì„¤ì •                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ retrieval_node                              â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ 1. OpenAI Embedding API í˜¸ì¶œ                â”‚
â”‚    query â†’ vector (1536 ì°¨ì›)               â”‚
â”‚ 2. Qdrant Vector ê²€ìƒ‰                       â”‚
â”‚    - HNSW ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©                      â”‚
â”‚    - top_k=10, similarity > 0.7            â”‚
â”‚ 3. state["retrieved_docs"] ì„¤ì •            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ reranking_node                              â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ - Cross-encoderë¡œ ì¬ìˆœìœ„í™” (ì„ íƒì )          â”‚
â”‚ - ì¤‘ë³µ ë¬¸ì„œ ì œê±°                             â”‚
â”‚ - Top-3 ë¬¸ì„œë§Œ ì„ íƒ                          â”‚
â”‚ - state["retrieved_docs"] ì—…ë°ì´íŠ¸         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ synthesis_node                              â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ í•©ì„±       â”‚
â”‚ - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¶œì²˜, í˜ì´ì§€ ë²ˆí˜¸)          â”‚
â”‚ - í† í° ìˆ˜ ê³„ì‚° (4096 í† í° ì œí•œ)              â”‚
â”‚ - state["context"] ì„¤ì •                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ response_node â­ï¸ LLM ë‹µë³€ ìƒì„±              â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ 1. System Prompt êµ¬ì„±                       â”‚
â”‚ 2. User Prompt: context + query            â”‚
â”‚ 3. OpenAI GPT-OSS-20B API í˜¸ì¶œ (await)     â”‚
â”‚ 4. ì‘ë‹µ í›„ì²˜ë¦¬ (ì¶œì²˜ ì¸ìš© ì¶”ê°€)               â”‚
â”‚ 5. state["response"] ì„¤ì •                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ history_node                                â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ - Django APIë¡œ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥             â”‚
â”‚ - í† í° ì‚¬ìš©ëŸ‰, íƒ€ì´ë° ì •ë³´ ì €ì¥               â”‚
â”‚ - state["metadata"] ì—…ë°ì´íŠ¸                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
                 END
```

### ì›Œí¬í”Œë¡œìš° ì½”ë“œ ì˜ˆì‹œ

```python
# app/main.py
from langgraph.graph import StateGraph
from app.nodes.graph_state.schemas import ChatState
from app.nodes.chat_process.user_input_node import user_input_node
from app.nodes.source_process.retrieval_node import retrieval_node
from app.nodes.source_process.reranking_node import reranking_node
from app.nodes.source_process.synthesis_node import synthesis_node
from app.nodes.chat_process.response_node import response_node
from app.nodes.chat_process.history_node import history_node

# LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
workflow = StateGraph(ChatState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("user_input", user_input_node)
workflow.add_node("retrieval", retrieval_node)
workflow.add_node("reranking", reranking_node)
workflow.add_node("synthesis", synthesis_node)
workflow.add_node("response", response_node)
workflow.add_node("history", history_node)

# ì—£ì§€ ì—°ê²° (ìˆœì„œ ì •ì˜)
workflow.set_entry_point("user_input")
workflow.add_edge("user_input", "retrieval")
workflow.add_edge("retrieval", "reranking")
workflow.add_edge("reranking", "synthesis")
workflow.add_edge("synthesis", "response")
workflow.add_edge("response", "history")
workflow.set_finish_point("history")

# ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
app = workflow.compile()
```

## ğŸ“Š GraphState ìŠ¤í‚¤ë§ˆ

### ChatState ì •ì˜

```python
# app/nodes/graph_state/schemas.py
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph import add_messages

class ChatState(TypedDict):
    """
    LangGraph ì›Œí¬í”Œë¡œìš° ì „ì²´ì—ì„œ ê³µìœ ë˜ëŠ” ìƒíƒœ ìŠ¤í‚¤ë§ˆ

    ê° ë…¸ë“œëŠ” ì´ ìƒíƒœì˜ ì¼ë¶€ë¥¼ ì½ê³  ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    TypedDictë¥¼ ì‚¬ìš©í•˜ì—¬ íƒ€ì… ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """

    # ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (LangChain ë©”ì‹œì§€ í˜•ì‹)
    # add_messages ë¦¬ë“€ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ìë™ ë³‘í•©
    messages: Annotated[Sequence[BaseMessage], add_messages]

    # ì‚¬ìš©ì ì¿¼ë¦¬ (ì›ë³¸ í…ìŠ¤íŠ¸)
    query: str

    # Vector DBì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    # ê° ë¬¸ì„œëŠ” {content, metadata, score} êµ¬ì¡°
    retrieved_docs: list[dict]

    # í•©ì„±ëœ ì»¨í…ìŠ¤íŠ¸ (LLMì— ì „ë‹¬í•  ë¬¸ë§¥)
    context: str

    # LLM ìƒì„± ì‘ë‹µ
    response: str

    # ë©”íƒ€ë°ì´í„° (í† í° ì‚¬ìš©ëŸ‰, íƒ€ì´ë°, ëª¨ë¸ ì •ë³´ ë“±)
    metadata: dict
```

### State ì—…ë°ì´íŠ¸ ì˜ˆì‹œ

```python
# ë…¸ë“œì—ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸
async def my_node(state: ChatState) -> ChatState:
    # ì¼ë¶€ í•„ë“œë§Œ ë°˜í™˜í•˜ë©´ ìë™ìœ¼ë¡œ ë³‘í•©ë¨
    return {
        "retrieved_docs": [{"content": "...", "metadata": {...}}],
        "metadata": {"retrieval_time_ms": 150}
    }
```

## ğŸ¯ í•µì‹¬ ë…¸ë“œ ìƒì„¸ ì„¤ëª…

### 1. user_input_node (ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬)

**ìœ„ì¹˜**: `app/nodes/chat_process/user_input_node.py`

**ì—­í• **:
- ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ (ë¹ˆ ë¬¸ìì—´, ê¸¸ì´ ì œí•œ, íŠ¹ìˆ˜ë¬¸ì)
- SQL Injection, XSS ê³µê²© íŒ¨í„´ íƒì§€ ë° ì°¨ë‹¨
- ì¿¼ë¦¬ íƒ€ì… ë¶„ì„ (ì‚¬ì‹¤í˜• ì§ˆë¬¸ vs ì˜ê²¬í˜• ì§ˆë¬¸)
- ì…ë ¥ ì •ê·œí™” (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜ ë“±)

**ì…ë ¥**:
- `state["messages"]`: ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬
- ì‚¬ìš©ì ìµœì‹  ë©”ì‹œì§€

**ì¶œë ¥**:
- `state["query"]`: ê²€ì¦ë˜ê³  ì •ê·œí™”ëœ ì¿¼ë¦¬
- `state["metadata"]["query_type"]`: ì¿¼ë¦¬ íƒ€ì… (factual/opinion)

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
from app.nodes.graph_state.schemas import ChatState
from app.utils.validators import validate_input, detect_injection
from app.utils.logger import logger

async def user_input_node(state: ChatState) -> ChatState:
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ê²€ì¦í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œ

    Args:
        state: ChatState (messages í•„ë“œ í•„ìˆ˜)

    Returns:
        ChatState: query, metadata ì—…ë°ì´íŠ¸

    Raises:
        ValueError: ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•Šì„ ê²½ìš°
    """
    # ìµœì‹  ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
    user_message = state["messages"][-1].content

    logger.info("Processing user input", message_length=len(user_message))

    # 1. ê¸°ë³¸ ê²€ì¦
    if not user_message or len(user_message.strip()) == 0:
        raise ValueError("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    if len(user_message) > 1000:
        raise ValueError("ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ìµœëŒ€ 1000ì)")

    # 2. ë³´ì•ˆ ê²€ì¦
    if detect_injection(user_message):
        raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤")

    # 3. ì •ê·œí™”
    query = user_message.strip()

    # 4. ì¿¼ë¦¬ íƒ€ì… ë¶„ì„
    query_type = analyze_query_type(query)

    logger.info("Input validated", query_type=query_type)

    return {
        "query": query,
        "metadata": {
            "query_type": query_type,
            "original_length": len(user_message)
        }
    }

def analyze_query_type(query: str) -> str:
    """ì¿¼ë¦¬ íƒ€ì… ë¶„ì„ (ì‚¬ì‹¤í˜•/ì˜ê²¬í˜•)"""
    factual_keywords = ["ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€"]
    opinion_keywords = ["ìƒê°", "ì˜ê²¬", "ì¶”ì²œ", "ì–´ë–¤ê°€"]

    query_lower = query.lower()

    if any(kw in query_lower for kw in factual_keywords):
        return "factual"
    elif any(kw in query_lower for kw in opinion_keywords):
        return "opinion"
    else:
        return "general"
```

### 2. retrieval_node (Vector ê²€ìƒ‰)

**ìœ„ì¹˜**: `app/nodes/source_process/retrieval_node.py`

**ì—­í• **:
- ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (OpenAI Embedding API)
- Qdrantì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (HNSW ì•Œê³ ë¦¬ì¦˜)
- Top-K ë¬¸ì„œ ë°˜í™˜ (ê¸°ë³¸ 10ê°œ, similarity > 0.7)

**ì…ë ¥**:
- `state["query"]`: ì‚¬ìš©ì ì¿¼ë¦¬

**ì¶œë ¥**:
- `state["retrieved_docs"]`: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
from app.nodes.graph_state.schemas import ChatState
from app.services.factory import get_embedding_service, get_vector_store
from app.utils.logger import logger
import time

async def retrieval_node(state: ChatState) -> ChatState:
    """
    Vector DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ

    Args:
        state: ChatState (query í•„ë“œ í•„ìˆ˜)

    Returns:
        ChatState: retrieved_docs ì—…ë°ì´íŠ¸
    """
    query = state["query"]
    logger.info("Starting retrieval", query=query)

    start_time = time.time()

    # 1. ì„ë² ë”© ìƒì„± (OpenAI text-embedding-3-large)
    embedding_service = get_embedding_service()
    query_vector = await embedding_service.embed(query)

    logger.debug("Embedding generated", vector_dim=len(query_vector))

    # 2. Vector ê²€ìƒ‰ (Qdrant)
    vector_store = get_vector_store()
    search_results = await vector_store.search(
        collection_name="documents",
        query_vector=query_vector,
        top_k=10,
        score_threshold=0.7  # ìœ ì‚¬ë„ ì„ê³„ê°’
    )

    # 3. ê²°ê³¼ í¬ë§·íŒ…
    retrieved_docs = []
    for result in search_results:
        retrieved_docs.append({
            "content": result.payload["text"],
            "metadata": {
                "filename": result.payload["filename"],
                "page": result.payload.get("page", 0),
                "chunk_id": result.id,
                "score": result.score
            }
        })

    elapsed_ms = int((time.time() - start_time) * 1000)
    logger.info("Retrieval completed",
                docs_found=len(retrieved_docs),
                elapsed_ms=elapsed_ms)

    return {
        "retrieved_docs": retrieved_docs,
        "metadata": {
            "retrieval_time_ms": elapsed_ms,
            "docs_retrieved": len(retrieved_docs)
        }
    }
```

### 3. reranking_node (ì¬ìˆœìœ„í™”)

**ìœ„ì¹˜**: `app/nodes/source_process/reranking_node.py`

**ì—­í• **:
- Cross-encoder ëª¨ë¸ë¡œ ì¿¼ë¦¬-ë¬¸ì„œ ê´€ë ¨ì„± ì¬í‰ê°€
- ì¤‘ë³µ ë¬¸ì„œ ì œê±° (ê°™ì€ íŒŒì¼ì˜ ì¸ì ‘ ì²­í¬)
- Top-3 ë¬¸ì„œë§Œ ì„ íƒí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ìµœì í™”

**ì…ë ¥**:
- `state["query"]`: ì‚¬ìš©ì ì¿¼ë¦¬
- `state["retrieved_docs"]`: ê²€ìƒ‰ëœ ë¬¸ì„œ (Top-10)

**ì¶œë ¥**:
- `state["retrieved_docs"]`: ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ (Top-3)

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
from app.nodes.graph_state.schemas import ChatState
from app.utils.logger import logger

async def reranking_node(state: ChatState) -> ChatState:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ìˆœìœ„í™”í•˜ê³  í•„í„°ë§í•˜ëŠ” ë…¸ë“œ

    Args:
        state: ChatState (query, retrieved_docs í•„ìˆ˜)

    Returns:
        ChatState: retrieved_docs ì—…ë°ì´íŠ¸ (Top-3)
    """
    query = state["query"]
    docs = state["retrieved_docs"]

    logger.info("Starting reranking", initial_docs=len(docs))

    if len(docs) == 0:
        logger.warning("No documents to rerank")
        return {"retrieved_docs": []}

    # 1. ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ì˜ ì¸ì ‘ ì²­í¬)
    deduplicated_docs = remove_duplicates(docs)

    # 2. Cross-encoder ì¬ìˆœìœ„í™” (í–¥í›„ êµ¬í˜„)
    # reranked_docs = await cross_encoder_rerank(query, deduplicated_docs)

    # í˜„ì¬ëŠ” ìŠ¤ì½”ì–´ ê¸°ì¤€ ì •ë ¬
    sorted_docs = sorted(
        deduplicated_docs,
        key=lambda x: x["metadata"]["score"],
        reverse=True
    )

    # 3. Top-3 ì„ íƒ
    top_docs = sorted_docs[:3]

    logger.info("Reranking completed", final_docs=len(top_docs))

    return {"retrieved_docs": top_docs}


def remove_duplicates(docs: list[dict]) -> list[dict]:
    """ì¸ì ‘ ì²­í¬ ì¤‘ë³µ ì œê±°"""
    seen_files = set()
    unique_docs = []

    for doc in docs:
        file_key = doc["metadata"]["filename"]
        if file_key not in seen_files:
            unique_docs.append(doc)
            seen_files.add(file_key)

    return unique_docs
```

### 4. synthesis_node (ì»¨í…ìŠ¤íŠ¸ í•©ì„±)

**ìœ„ì¹˜**: `app/nodes/source_process/synthesis_node.py`

**ì—­í• **:
- ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ í•©ì„±
- ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° í¬ë§·íŒ… (ì¶œì²˜, í˜ì´ì§€)
- í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ (ìµœëŒ€ 4096 í† í°)

**ì…ë ¥**:
- `state["retrieved_docs"]`: ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ

**ì¶œë ¥**:
- `state["context"]`: í•©ì„±ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
from app.nodes.graph_state.schemas import ChatState
from app.utils.logger import logger
import tiktoken

async def synthesis_node(state: ChatState) -> ChatState:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ í•©ì„±í•˜ëŠ” ë…¸ë“œ

    Args:
        state: ChatState (retrieved_docs í•„ìˆ˜)

    Returns:
        ChatState: context ì—…ë°ì´íŠ¸
    """
    docs = state["retrieved_docs"]

    logger.info("Starting context synthesis", docs_count=len(docs))

    if len(docs) == 0:
        return {
            "context": "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
            "metadata": {"context_tokens": 0}
        }

    # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, doc in enumerate(docs, 1):
        filename = doc["metadata"]["filename"]
        page = doc["metadata"]["page"]
        content = doc["content"]

        context_parts.append(
            f"[ë¬¸ì„œ {i}: {filename}, p.{page}]\n{content}\n"
        )

    context = "\n---\n".join(context_parts)

    # 2. í† í° ìˆ˜ ê³„ì‚°
    token_count = count_tokens(context)

    # 3. í† í° ì œí•œ (4096 í† í°)
    if token_count > 4096:
        logger.warning("Context too long, truncating",
                      original_tokens=token_count)
        context = truncate_context(context, max_tokens=4096)
        token_count = 4096

    logger.info("Context synthesized", tokens=token_count)

    return {
        "context": context,
        "metadata": {"context_tokens": token_count}
    }


def count_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚° (tiktoken)"""
    encoding = tiktoken.encoding_for_model("gpt-4")
    return len(encoding.encode(text))


def truncate_context(text: str, max_tokens: int) -> str:
    """í† í° ìˆ˜ ì œí•œì— ë§ê²Œ í…ìŠ¤íŠ¸ ì˜ë¼ë‚´ê¸°"""
    encoding = tiktoken.encoding_for_model("gpt-4")
    tokens = encoding.encode(text)
    truncated_tokens = tokens[:max_tokens]
    return encoding.decode(truncated_tokens)
```

### 5. response_node â­ï¸ (LLM ë‹µë³€ ìƒì„±)

**ìœ„ì¹˜**: `app/nodes/chat_process/response_node.py`

**ì—­í• **:
- **ìµœì¢… ë‹µë³€ ìƒì„±** (OpenAI GPT-OSS-20B í˜¸ì¶œ)
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
- ì‘ë‹µ í›„ì²˜ë¦¬ (ì¶œì²˜ ì¸ìš© ì¶”ê°€, í¬ë§·íŒ…)

**ì…ë ¥**:
- `state["context"]`: í•©ì„±ëœ ì»¨í…ìŠ¤íŠ¸
- `state["query"]`: ì‚¬ìš©ì ì¿¼ë¦¬

**ì¶œë ¥**:
- `state["response"]`: LLM ìƒì„± ì‘ë‹µ

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
from app.nodes.graph_state.schemas import ChatState
from app.services.factory import get_llm_service
from app.utils.logger import logger
import time

async def response_node(state: ChatState) -> ChatState:
    """
    â­ï¸ LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ

    ì´ ë…¸ë“œì—ì„œ OpenAI GPT-OSS-20B APIë¥¼ í˜¸ì¶œí•˜ì—¬
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        state: ChatState (context, query í•„ìˆ˜)

    Returns:
        ChatState: response, metadata ì—…ë°ì´íŠ¸
    """
    context = state["context"]
    query = state["query"]

    logger.info("Starting response generation", query=query)

    start_time = time.time()

    # 1. LLM ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° (Dependency Injection)
    llm_service = get_llm_service()

    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ Contextë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. Contextì— ìˆëŠ” ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. Contextì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”
4. ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
"""

    # 3. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""Context:
{context}

Question: {query}

Answer:"""

    # 4. LLM API í˜¸ì¶œ â­ï¸ ì‹¤ì œ LLM í˜¸ì¶œ ì§€ì 
    try:
        response = await llm_service.generate(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=0.7,
            max_tokens=1000,
            top_p=0.9
        )

        elapsed_ms = int((time.time() - start_time) * 1000)

        logger.info("Response generated successfully",
                   response_length=len(response),
                   elapsed_ms=elapsed_ms,
                   tokens_used=llm_service.last_token_count)

        # 5. ì‘ë‹µ í›„ì²˜ë¦¬ (ì¶œì²˜ ì¸ìš© ì¶”ê°€)
        processed_response = add_citations(
            response=response,
            sources=state["retrieved_docs"]
        )

        return {
            "response": processed_response,
            "metadata": {
                "model": "gpt-oss-20b",
                "tokens_used": llm_service.last_token_count,
                "generation_time_ms": elapsed_ms,
                "temperature": 0.7
            }
        }

    except Exception as e:
        logger.error("LLM generation failed", error=str(e))
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "metadata": {"error": str(e)}
        }


def add_citations(response: str, sources: list[dict]) -> str:
    """
    ì‘ë‹µì— ì¶œì²˜ ì¸ìš© ì¶”ê°€

    Args:
        response: LLM ìƒì„± ì‘ë‹µ
        sources: ì°¸ê³ í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì¶œì²˜ê°€ í¬í•¨ëœ ì‘ë‹µ
    """
    if not sources:
        return response

    citations = "\n\n**ğŸ“š ì°¸ê³  ë¬¸ì„œ:**\n"
    for i, doc in enumerate(sources, 1):
        filename = doc["metadata"]["filename"]
        page = doc["metadata"]["page"]
        score = doc["metadata"]["score"]
        citations += f"{i}. {filename} (p.{page}, ìœ ì‚¬ë„: {score:.2f})\n"

    return response + citations
```

### 6. history_node (íˆìŠ¤í† ë¦¬ ì €ì¥)

**ìœ„ì¹˜**: `app/nodes/chat_process/history_node.py`

**ì—­í• **:
- Django APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥
- í† í° ì‚¬ìš©ëŸ‰, íƒ€ì´ë° ì •ë³´ ê¸°ë¡
- ì‚¬ìš©ìë³„ ì„¸ì…˜ ê´€ë¦¬

**ì…ë ¥**:
- `state["query"]`: ì‚¬ìš©ì ì§ˆë¬¸
- `state["response"]`: LLM ì‘ë‹µ
- `state["metadata"]`: ë©”íƒ€ë°ì´í„°

**ì¶œë ¥**:
- ìƒíƒœ ì—…ë°ì´íŠ¸ ì—†ìŒ (side effectë§Œ ìˆ˜í–‰)

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
from app.nodes.graph_state.schemas import ChatState
from app.utils.logger import logger
import httpx

async def history_node(state: ChatState) -> ChatState:
    """
    ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ Django APIì— ì €ì¥í•˜ëŠ” ë…¸ë“œ

    Args:
        state: ChatState (ì „ì²´ ìƒíƒœ í•„ìš”)

    Returns:
        ChatState: ìƒíƒœ ë³€ê²½ ì—†ìŒ
    """
    logger.info("Saving chat history")

    # Django API ì—”ë“œí¬ì¸íŠ¸
    django_url = "http://django:8001/api/chat-history/messages/"

    # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
    history_data = {
        "session_id": state.get("session_id", "default"),
        "messages": [
            {
                "role": "user",
                "content": state["query"]
            },
            {
                "role": "assistant",
                "content": state["response"],
                "metadata": state["metadata"]
            }
        ]
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(django_url, json=history_data)
            response.raise_for_status()

        logger.info("History saved successfully")

    except Exception as e:
        logger.error("Failed to save history", error=str(e))
        # íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ

    return {}  # ìƒíƒœ ë³€ê²½ ì—†ìŒ
```

## ğŸ”Œ ì„œë¹„ìŠ¤ ê³„ì¸µ (Port-Adapter íŒ¨í„´)

### Port (ì¸í„°í˜ì´ìŠ¤)

**ìœ„ì¹˜**: `app/services/interfaces/llm_interface.py`

```python
from abc import ABC, abstractmethod
from typing import AsyncGenerator

class LLMInterface(ABC):
    """
    LLM ì„œë¹„ìŠ¤ ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤ (Port)

    êµ¬í˜„ì²´ëŠ” OpenAI, Anthropic, ë¡œì»¬ ëª¨ë¸ ë“± ë‹¤ì–‘í•  ìˆ˜ ìˆìŒ
    """

    @abstractmethod
    async def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        top_p: float = 1.0
    ) -> str:
        """
        ë‹µë³€ ìƒì„± (ë™ê¸°)

        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            temperature: ìƒì„± ë‹¤ì–‘ì„± (0.0-2.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            top_p: Nucleus sampling

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        pass

    @abstractmethod
    async def stream(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±

        Yields:
            ìƒì„±ëœ ë‹µë³€ ì²­í¬
        """
        pass

    @property
    @abstractmethod
    def last_token_count(self) -> int:
        """ë§ˆì§€ë§‰ í˜¸ì¶œì˜ í† í° ì‚¬ìš©ëŸ‰"""
        pass
```

### Adapter (êµ¬í˜„ì²´)

**ìœ„ì¹˜**: `app/services/implementations/openai_llm_service.py`

```python
from openai import AsyncOpenAI
from app.services.interfaces.llm_interface import LLMInterface
from config.settings import settings
from app.utils.logger import logger
from typing import AsyncGenerator

class OpenAILLMService(LLMInterface):
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•œ LLM ì„œë¹„ìŠ¤ êµ¬í˜„ì²´ (Adapter)
    """

    def __init__(self, api_key: str = None):
        """
        Args:
            api_key: OpenAI API Key (ì—†ìœ¼ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)
        """
        self.api_key = api_key or settings.OPENAI_API_KEY
        self.client = AsyncOpenAI(api_key=self.api_key)
        self.model = settings.OPENAI_MODEL  # "gpt-oss-20b"
        self._last_token_count = 0

    async def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        top_p: float = 1.0
    ) -> str:
        """OpenAI Chat Completion API í˜¸ì¶œ"""

        logger.debug("Calling OpenAI API", model=self.model)

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p
            )

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥
            self._last_token_count = response.usage.total_tokens

            # ì‘ë‹µ ì¶”ì¶œ
            content = response.choices[0].message.content

            logger.debug("OpenAI API call succeeded",
                        tokens=self._last_token_count)

            return content

        except Exception as e:
            logger.error("OpenAI API call failed", error=str(e))
            raise

    async def stream(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> AsyncGenerator[str, None]:
        """OpenAI ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ"""

        logger.debug("Starting OpenAI stream", model=self.model)

        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            logger.error("OpenAI stream failed", error=str(e))
            raise

    @property
    def last_token_count(self) -> int:
        return self._last_token_count
```

### Dependency Injection (Factory)

**ìœ„ì¹˜**: `app/services/factory.py`

```python
from app.services.interfaces.llm_interface import LLMInterface
from app.services.interfaces.embedding_interface import EmbeddingInterface
from app.services.interfaces.vector_store_interface import VectorStoreInterface
from app.services.implementations.openai_llm_service import OpenAILLMService
from app.services.implementations.openai_embedding_service import OpenAIEmbeddingService
from app.services.implementations.qdrant_service import QdrantService
from config.settings import settings

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
_llm_service = None
_embedding_service = None
_vector_store = None


def get_llm_service() -> LLMInterface:
    """
    LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜

    í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ êµ¬í˜„ì²´ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŒ:
    - í”„ë¡œë•ì…˜: OpenAI
    - í…ŒìŠ¤íŠ¸: Mock
    """
    global _llm_service

    if _llm_service is None:
        _llm_service = OpenAILLMService(
            api_key=settings.OPENAI_API_KEY
        )

    return _llm_service


def get_embedding_service() -> EmbeddingInterface:
    """Embedding ì„œë¹„ìŠ¤ íŒ©í† ë¦¬"""
    global _embedding_service

    if _embedding_service is None:
        _embedding_service = OpenAIEmbeddingService(
            api_key=settings.OPENAI_API_KEY
        )

    return _embedding_service


def get_vector_store() -> VectorStoreInterface:
    """Vector Store ì„œë¹„ìŠ¤ íŒ©í† ë¦¬"""
    global _vector_store

    if _vector_store is None:
        _vector_store = QdrantService(
            host=settings.QDRANT_HOST,
            port=settings.QDRANT_PORT
        )

    return _vector_store
```

## ğŸš€ API ì‚¬ìš©ë²•

### FastAPI ì—”ë“œí¬ì¸íŠ¸

**1. ì±„íŒ… (ë™ê¸°)**

```bash
curl -X POST http://localhost:8000/chat/invoke \
  -H "Content-Type: application/json" \
  -d '{
    "query": "RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?",
    "session_id": "user123"
  }'
```

**ì‘ë‹µ**:
```json
{
  "response": "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ...\n\n**ğŸ“š ì°¸ê³  ë¬¸ì„œ:**\n1. rag_intro.pdf (p.1, ìœ ì‚¬ë„: 0.92)",
  "metadata": {
    "model": "gpt-oss-20b",
    "tokens_used": 450,
    "generation_time_ms": 2300,
    "retrieval_time_ms": 150
  }
}
```

**2. ì±„íŒ… (ìŠ¤íŠ¸ë¦¬ë°)**

```bash
curl -X POST http://localhost:8000/chat/stream \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -d '{
    "query": "RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?"
  }'
```

**ì‘ë‹µ (SSE)**:
```
data: {"chunk": "RAGëŠ” "}
data: {"chunk": "Retrieval-"}
data: {"chunk": "Augmented "}
data: {"chunk": "Generation"}
data: {"done": true, "metadata": {...}}
```

**3. ë¬¸ì„œ ì—…ë¡œë“œ**

```bash
curl -X POST http://localhost:8000/documents/upload \
  -F "file=@document.pdf"
```

**ì‘ë‹µ**:
```json
{
  "document_id": "doc_12345",
  "filename": "document.pdf",
  "page_count": 10,
  "chunks_created": 45,
  "status": "processed"
}
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Node)

```python
# tests/unit/nodes/test_response_node.py
import pytest
from app.nodes.chat_process.response_node import response_node
from app.nodes.graph_state.schemas import ChatState

@pytest.mark.asyncio
async def test_response_node_success():
    """response_nodeê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""

    # Given: í…ŒìŠ¤íŠ¸ ìƒíƒœ
    state = ChatState(
        query="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤",
        context="í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤",
        retrieved_docs=[
            {
                "content": "ë¬¸ì„œ ë‚´ìš©",
                "metadata": {"filename": "test.pdf", "page": 1, "score": 0.9}
            }
        ],
        messages=[],
        response="",
        metadata={}
    )

    # When: ë…¸ë“œ ì‹¤í–‰
    result = await response_node(state)

    # Then: ê²€ì¦
    assert "response" in result
    assert len(result["response"]) > 0
    assert "metadata" in result
    assert result["metadata"]["model"] == "gpt-oss-20b"
    assert result["metadata"]["tokens_used"] > 0
```

### Mock ì„œë¹„ìŠ¤ ì‚¬ìš©

```python
# tests/conftest.py
import pytest
from app.services.interfaces.llm_interface import LLMInterface

class MockLLMService(LLMInterface):
    """í…ŒìŠ¤íŠ¸ìš© Mock LLM ì„œë¹„ìŠ¤"""

    async def generate(self, **kwargs) -> str:
        return "ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤"

    async def stream(self, **kwargs):
        yield "ëª¨ì˜ "
        yield "ìŠ¤íŠ¸ë¦¬ë° "
        yield "ì‘ë‹µ"

    @property
    def last_token_count(self) -> int:
        return 100

@pytest.fixture
def mock_llm_service(monkeypatch):
    """LLM ì„œë¹„ìŠ¤ë¥¼ Mockìœ¼ë¡œ êµì²´"""
    monkeypatch.setattr(
        "app.services.factory.get_llm_service",
        lambda: MockLLMService()
    )
```

### í†µí•© í…ŒìŠ¤íŠ¸ (Workflow)

```python
# tests/integration/test_workflow.py
import pytest
from app.main import app as langgraph_app
from langchain_core.messages import HumanMessage

@pytest.mark.asyncio
async def test_full_workflow():
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""

    # Given: ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "messages": [HumanMessage(content="RAGë€ ë¬´ì—‡ì¸ê°€ìš”?")],
        "query": "",
        "retrieved_docs": [],
        "context": "",
        "response": "",
        "metadata": {}
    }

    # When: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = await langgraph_app.ainvoke(initial_state)

    # Then: ê²€ì¦
    assert result["response"] != ""
    assert len(result["retrieved_docs"]) > 0
    assert result["context"] != ""
    assert result["metadata"]["tokens_used"] > 0
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ë¹„ë™ê¸° ì²˜ë¦¬

ëª¨ë“  I/O ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ì—¬ ë™ì‹œì„± í™•ë³´:

```python
# âŒ ë™ê¸° ë°©ì‹ (ë¸”ë¡œí‚¹)
def slow_node(state):
    result = requests.get("http://api.com")  # ë¸”ë¡œí‚¹
    return {"data": result.json()}

# âœ… ë¹„ë™ê¸° ë°©ì‹ (ë…¼ë¸”ë¡œí‚¹)
async def fast_node(state):
    async with httpx.AsyncClient() as client:
        result = await client.get("http://api.com")  # ë…¼ë¸”ë¡œí‚¹
    return {"data": result.json()}
```

### 2. Connection Pooling

```python
# config/database_config.py
from qdrant_client import AsyncQdrantClient

# Qdrant í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©
qdrant_client = AsyncQdrantClient(
    host="localhost",
    port=6333,
    timeout=60,  # ì—°ê²° íƒ€ì„ì•„ì›ƒ
    prefer_grpc=True  # gRPC ì‚¬ìš© (ë” ë¹ ë¦„)
)
```

### 3. ìºì‹±

```python
from functools import lru_cache
import hashlib

# ì„ë² ë”© ìºì‹± (ê°™ì€ ì¿¼ë¦¬ ë°˜ë³µ ì‹œ ì¬ì‚¬ìš©)
_embedding_cache = {}

async def get_embedding_cached(text: str):
    cache_key = hashlib.md5(text.encode()).hexdigest()

    if cache_key in _embedding_cache:
        return _embedding_cache[cache_key]

    embedding = await embedding_service.embed(text)
    _embedding_cache[cache_key] = embedding

    return embedding
```

## ğŸ”§ í™•ì¥ ê°€ì´ë“œ

### ìƒˆ ë…¸ë“œ ì¶”ê°€ (3ë‹¨ê³„)

**1ë‹¨ê³„: ë…¸ë“œ íŒŒì¼ ìƒì„±**

```python
# app/nodes/custom/my_custom_node.py
from app.nodes.graph_state.schemas import ChatState
from app.utils.logger import logger

async def my_custom_node(state: ChatState) -> ChatState:
    """
    ì»¤ìŠ¤í…€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ

    ì˜ˆ: ì¿¼ë¦¬ í™•ì¥, ë²ˆì—­, ê°ì • ë¶„ì„ ë“±
    """
    logger.info("Running custom node")

    query = state["query"]

    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„
    expanded_query = await expand_query(query)

    return {
        "query": expanded_query,
        "metadata": {"query_expanded": True}
    }

async def expand_query(query: str) -> str:
    """ì¿¼ë¦¬ í™•ì¥ ë¡œì§"""
    # ì˜ˆ: ë™ì˜ì–´ ì¶”ê°€, ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
    return query + " (ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨)"
```

**2ë‹¨ê³„: ì›Œí¬í”Œë¡œìš°ì— ë…¸ë“œ ì¶”ê°€**

```python
# app/main.py
from app.nodes.custom.my_custom_node import my_custom_node

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("custom", my_custom_node)

# ì—£ì§€ ì—°ê²° (user_input â†’ custom â†’ retrieval)
workflow.add_edge("user_input", "custom")
workflow.add_edge("custom", "retrieval")
```

**3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì‘ì„±**

```python
# tests/unit/nodes/test_my_custom_node.py
import pytest
from app.nodes.custom.my_custom_node import my_custom_node

@pytest.mark.asyncio
async def test_my_custom_node():
    state = {"query": "í…ŒìŠ¤íŠ¸"}
    result = await my_custom_node(state)
    assert result["metadata"]["query_expanded"] is True
```

### ìƒˆ ì„œë¹„ìŠ¤ ì¶”ê°€ (4ë‹¨ê³„)

**1ë‹¨ê³„: ì¸í„°í˜ì´ìŠ¤ ì •ì˜**

```python
# app/services/interfaces/translator_interface.py
from abc import ABC, abstractmethod

class TranslatorInterface(ABC):
    @abstractmethod
    async def translate(self, text: str, target_lang: str) -> str:
        pass
```

**2ë‹¨ê³„: êµ¬í˜„ì²´ ì‘ì„±**

```python
# app/services/implementations/google_translator.py
from app.services.interfaces.translator_interface import TranslatorInterface

class GoogleTranslator(TranslatorInterface):
    async def translate(self, text: str, target_lang: str) -> str:
        # Google Translate API í˜¸ì¶œ
        return f"Translated: {text}"
```

**3ë‹¨ê³„: Factoryì— ë“±ë¡**

```python
# app/services/factory.py
def get_translator_service() -> TranslatorInterface:
    return GoogleTranslator()
```

**4ë‹¨ê³„: ë…¸ë“œì—ì„œ ì‚¬ìš©**

```python
from app.services.factory import get_translator_service

async def translation_node(state: ChatState) -> ChatState:
    translator = get_translator_service()
    translated = await translator.translate(state["query"], "en")
    return {"query": translated}
```

## ğŸ“– ì°¸ê³  ìë£Œ

### LangGraph
- [LangGraph ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/docs/langgraph)
- [LangGraph íŠœí† ë¦¬ì–¼](https://langchain-ai.github.io/langgraph/tutorials/)
- [StateGraph API ë ˆí¼ëŸ°ìŠ¤](https://langchain-ai.github.io/langgraph/reference/graphs/)

### Clean Architecture
- [Clean Architecture (Uncle Bob)](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)
- [Port-Adapter íŒ¨í„´ (Hexagonal Architecture)](https://herbertograca.com/2017/11/16/explicit-architecture-01-ddd-hexagonal-onion-clean-cqrs-how-i-put-it-all-together/)

### ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°
- [Python asyncio ê³µì‹ ë¬¸ì„œ](https://docs.python.org/3/library/asyncio.html)
- [Real Python - Async IO in Python](https://realpython.com/async-io-python/)

---

**ì´ ë¬¸ì„œëŠ” `app/` í´ë”ì˜ LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì´í•´í•˜ê³  í™•ì¥í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.**
